{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Rental Property Dataset Using Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team Members:<br>\n",
    "Madhu Bandru - MB4236<br>\n",
    "Varun Kumar B - VB475<br>\n",
    "Karthikreddy Kuna - KK3375<br>\n",
    "Pradeep Kumar Kankipati - PK593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import requests, re, pandas\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/atlanta-ga/',\n",
       " '/austin-tx/',\n",
       " '/baltimore-md/',\n",
       " '/boston-ma/',\n",
       " '/chicago-il/',\n",
       " '/cincinnati-oh/',\n",
       " '/dallas-tx/',\n",
       " '/denver-co/',\n",
       " '/detroit-mi/',\n",
       " '/houston-tx/',\n",
       " '/irvine-ca/',\n",
       " '/los-angeles-ca/',\n",
       " '/miami-fl/',\n",
       " '/nashville-tn/',\n",
       " '/nj/',\n",
       " '/new-york-ny/',\n",
       " '/orlando-fl/',\n",
       " '/philadelphia-pa/',\n",
       " '/phoenix-az/',\n",
       " '/portland-or/',\n",
       " '/queens-ny/',\n",
       " '/san-antonio-tx/',\n",
       " '/san-diego-ca/',\n",
       " '/san-francisco-ca/',\n",
       " '/seattle-wa/',\n",
       " '/tampa-fl/',\n",
       " '/washington-dc/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting top market cities and their states\n",
    "\n",
    "base_url = \"https://www.apartments.com/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "response = requests.get(base_url, headers=headers)\n",
    "soup1 = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "city_url = soup1.find_all(\"div\",{\"class\":\"clearfix\"})\n",
    "list_city=[]\n",
    "for i in city_url:\n",
    "     data=i.find_all(\"li\")\n",
    "for j in data:\n",
    "    if re.search(r'Pet',j.find(\"a\")[\"title\"]):\n",
    "        break\n",
    "    else:\n",
    "        list_city.append(j.find(\"a\")[\"href\"])\n",
    "list_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracting individual property URL's for a city\n",
    "\n",
    "\n",
    "def get_url_list(each_city):\n",
    "    url = \"https://www.apartments.com\"+each_city\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    c=response.text\n",
    "    soup = BeautifulSoup(c,\"html.parser\")\n",
    "\n",
    "\n",
    "    ####get the last page number\n",
    "    num_list = ''\n",
    "    page_num = soup.find_all(\"nav\", {\"class\":\"paging\"})\n",
    "\n",
    "    for tags in page_num:\n",
    "        if tags.find('a'):\n",
    "            num_list += ((tags.text).strip())\n",
    "    page_num_list = num_list.replace(\"\\n\",\"\").replace(\".\",\"\")\n",
    "\n",
    "    last_page_num = page_num_list.split(' ')[len(page_num_list.split(' ')) - 1]\n",
    "\n",
    "    if last_page_num != '5Next':\n",
    "        last_page = int(last_page_num)\n",
    "    else:\n",
    "        page_num = soup.find_all('p', {'class' : \"searchResults\"})\n",
    "        for tags in page_num:\n",
    "            if tags.find('span'):\n",
    "                last_page = int(((tags.text).split(' '))[len((tags.text).split(' ')) - 1])\n",
    "\n",
    "    ####get urls of all properties in all pages\n",
    "    url_list = []\n",
    "    current_page = 1\n",
    "    while current_page < (last_page + 1):\n",
    "\n",
    "        if current_page == 1:\n",
    "            url = \"https://www.apartments.com\"+each_city\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "            response = requests.get(url, headers=headers)\n",
    "            c=response.text\n",
    "            soup = BeautifulSoup(c,\"html.parser\")\n",
    "        else:\n",
    "            url = \"https://www.apartments.com\"+each_city+str(current_page)+\"/\"\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "            response = requests.get(url, headers=headers)\n",
    "            c=response.text\n",
    "            soup = BeautifulSoup(c,\"html.parser\")\n",
    "\n",
    "        #appending property URL's to a list.\n",
    "        placard_header = soup.find_all(\"li\")\n",
    "        for data in placard_header:\n",
    "            if data.find('article'):\n",
    "                data1 = data.find('article')\n",
    "                url_link = data1.get('data-url')\n",
    "                if url_link == None:\n",
    "                    data1 = data.find('a')\n",
    "                    url_link = data1.get('href')\n",
    "                    url_list.append(url_link) \n",
    "                else:\n",
    "                    url_list.append(url_link)    \n",
    "        current_page += 1        \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.apartments.com/fallsview-gardens-philadelphia-pa/xk810sq/',\n",
       " 'https://www.apartments.com/hanover-north-broad-philadelphia-pa/540bw8m/',\n",
       " 'https://www.apartments.com/chestnut-hill-village-philadelphia-pa/4sbjxeh/',\n",
       " 'https://www.apartments.com/the-irvine-philadelphia-pa/61hve9v/',\n",
       " 'https://www.apartments.com/brookview-apartments-at-elkins-park-elkins-park-pa/ked8x69/',\n",
       " 'https://www.apartments.com/lynnewood-gardens-elkins-park-pa/7bjgd3y/',\n",
       " 'https://www.apartments.com/colonial-point-apartments-feasterville-trevose-pa/sd0qgc7/',\n",
       " 'https://www.apartments.com/tower-place-philadelphia-pa/bex449c/',\n",
       " 'https://www.apartments.com/rushwood-apartments-philadelphia-pa/g8qsrgz/',\n",
       " 'https://www.apartments.com/the-piazza-philadelphia-pa/pslb7ts/']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Provide the city and state(abbrevation) in city_state field.\n",
    "\n",
    "city_state = '/philadelphia-pa/'\n",
    "if city_state not in list_city:\n",
    "    print('Invalid city or state')\n",
    "else:\n",
    "    property_url_list = get_url_list(city_state)\n",
    "\n",
    "property_url_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracting required fields to build the data set for each property in the above list.\n",
    "\n",
    "property_details = defaultdict(dict)\n",
    "i = 0\n",
    "address_var_list = []\n",
    "for new_url in property_url_list:\n",
    "    i += 1\n",
    "\n",
    "    new_response = requests.get(new_url, headers=headers)\n",
    "    new_c=new_response.text\n",
    "    new_soup = BeautifulSoup(new_c,\"html.parser\")\n",
    "\n",
    "    #####Locate and store Property name in dictionary\n",
    "    property_name = (new_soup.find(\"h1\", {\"class\":\"propertyName\"}).text).strip()\n",
    "    property_details[i]['property name'] = property_name\n",
    "\n",
    "    #####Locate and store Property adress in dictionary\n",
    "    data1 = ''\n",
    "    property_address = new_soup.find_all(\"h2\")\n",
    "    for tags in property_address:\n",
    "        if tags.find('span'):\n",
    "            data1 += ((tags.text).strip())\n",
    "    data2 = (data1.replace(\"\\r\",\"\").replace(\"  \",\"\").replace(\",\",\"\")).split('\\n')\n",
    "\n",
    "    ####few properties have street name as property name.\n",
    "    if len(data2) == 4:\n",
    "        property_details[i]['property street'] = data2[0]\n",
    "        property_details[i]['property city'] = data2[1]\n",
    "        property_details[i]['property state'] = data2[2]\n",
    "        property_details[i]['property zipcode'] = data2[3]\n",
    "    else:\n",
    "        property_details[i]['property street'] = property_name\n",
    "        property_details[i]['property city'] = data2[0]\n",
    "        property_details[i]['property state'] = data2[1]\n",
    "        property_details[i]['property zipcode'] = data2[2]        \n",
    "\n",
    "    #####Locate and store Property features in dictionary\n",
    "    feature_data = ''\n",
    "    property_features = new_soup.find_all(\"div\", {\"class\":\"rentRollupContainer\"})\n",
    "    for tags in property_features:\n",
    "        if tags.find('span'):\n",
    "            feature_data += (tags.text).strip()\n",
    "\n",
    "    feature_data1 = (feature_data.replace(\"\\t\",\"\").replace(\"\\n\",\"\")).split(\"\\r\")\n",
    "    property_details[i]['Studio'] = 'NA'\n",
    "    property_details[i]['1 Bedroom'] = 'NA'\n",
    "    property_details[i]['2 Bedrooms'] = 'NA'\n",
    "    property_details[i]['3 Bedrooms'] = 'NA'\n",
    "    for tags in range(0, len(feature_data1), 3):\n",
    "        if feature_data1[tags] == 'StudioStudio':\n",
    "            property_details[i]['Studio'] = feature_data1[tags+2].strip()\n",
    "        elif feature_data1[tags] == '1 Bedroom1 Bed':\n",
    "            property_details[i]['1 Bedroom'] = (feature_data1[tags+2]).strip()\n",
    "        elif feature_data1[tags] == '2 Bedrooms2 Beds':\n",
    "            property_details[i]['2 Bedrooms'] = feature_data1[tags+2].strip()\n",
    "        elif feature_data1[tags] == '3 Bedrooms3 Beds':\n",
    "            property_details[i]['3 Bedrooms'] = feature_data1[tags+2].strip()\n",
    "\n",
    "    ####Latitude and Longitude of property.\n",
    "    latitude,longitude = new_soup.find_all(\"meta\", property={\"place:location:latitude\",\"place:location:longitude\"})\n",
    "    property_details[i]['Latitude'] = latitude[\"content\"]\n",
    "    property_details[i]['Longitude'] = longitude[\"content\"]    \n",
    "    \n",
    "    #####Append URL \n",
    "    property_details[i]['URL'] = new_url\n",
    "\n",
    "property_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the data in CSV file using pandas.\n",
    "\n",
    "df = pandas.DataFrame(property_details).T\n",
    "neworder = ['property name', 'property street','property city','property state','property zipcode','Studio', '1 Bedroom', \n",
    "            '2 Bedrooms', '3 Bedrooms', 'Latitude', 'Longitude', 'URL' ]\n",
    "df=df.reindex(columns=neworder)\n",
    "df.to_csv(\"data/rental_data_set.csv\", encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a51c545b4b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
